{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import math\n",
    "from random import randint\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.load('ja_ginza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ratio = 0.5\n",
    "span_length =3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for line in open('/home/chen/T5_mask_infilling/clinic_corpus.txt', 'r'):\n",
    "    if line is not None:\n",
    "        sentences.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11832"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
    "\n",
    "T5_PATH = '/home/chen/T5_mask_infilling/torch_model_clinic' \n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "\n",
    "\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(T5_PATH)\n",
    "t5_config = T5Config.from_pretrained(T5_PATH)\n",
    "t5_mlm = T5ForConditionalGeneration.from_pretrained(T5_PATH, config=t5_config).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 32/32 [01:33<00:00,  2.91s/it]\n"
     ]
    }
   ],
   "source": [
    "gen_list = []\n",
    "input_list = []\n",
    "for i in tqdm(range(len(sentences)-11800)):    \n",
    "    \n",
    "    doc = nlp(sentences[i])\n",
    "    mask_token = math.ceil(len(doc)*mask_ratio)\n",
    "    chunk_num = math.ceil(mask_token/span_length)\n",
    "    total_chunk = chunk_num\n",
    "    assert chunk_num != 0\n",
    "    \n",
    "    index_sequence = [i for i in range(len(doc)-span_length+1)]\n",
    "    for i in range(len(index_sequence)):\n",
    "        index_sequence[i] = [i for i in range(i,i+span_length)] \n",
    "        \n",
    "    mask_index = []\n",
    "    first_token_mask = False\n",
    "    last_token_mask = False\n",
    "    while chunk_num > 0:\n",
    "        index_sequence_length = len(index_sequence)\n",
    "        masked_spans = index_sequence[randint(0,index_sequence_length-1)]\n",
    "        if masked_spans[0] == 0:\n",
    "            first_token_mask = True\n",
    "        if masked_spans[0] ==  len(doc)-span_length:\n",
    "            last_token_mask = True\n",
    "        mask_index.extend(masked_spans)\n",
    "        index_id = index_sequence.index(masked_spans)\n",
    "        del index_sequence[index_id-span_length+1:index_id+span_length]\n",
    "        chunk_num = chunk_num - 1    \n",
    "        \n",
    "    mask_index.sort()\n",
    "    \n",
    "    text_sequence = [i for i in range(len(doc))]\n",
    "    unmask_index = list(set(text_sequence) - set(mask_index))\n",
    "    \n",
    "    unmask_index.sort()\n",
    "    \n",
    "    input_text = [None] * len(doc)\n",
    "    for i in unmask_index:\n",
    "        input_text[i] = doc[i]\n",
    "        \n",
    "        \n",
    "    re_input_text = []\n",
    "    for i in range(len(input_text)-1):\n",
    "        if str(input_text[i]) == 'None' and str(input_text[i+1]) == 'None':\n",
    "            pass\n",
    "        else:\n",
    "            re_input_text.append(input_text[i])\n",
    "        i+=2               \n",
    "\n",
    "    re_input_text.append(input_text[len(input_text)-1])   \n",
    "    \n",
    "    sent_token_id = 0\n",
    "    for i in range(len(re_input_text)):\n",
    "        if str(re_input_text[i]) == 'None':\n",
    "            re_input_text[i] = f'<extra_id_{sent_token_id}>'\n",
    "            sent_token_id += 1  \n",
    "            \n",
    "    inputs = ''.join(str(e) for e in re_input_text)\n",
    "    input_list.append(inputs)\n",
    "    text = inputs\n",
    "\n",
    "    encoded = t5_tokenizer(text, add_special_tokens=True, return_tensors='pt', \n",
    "                       padding = 'max_length', truncation = True, max_length = 128)\n",
    "    input_ids = encoded['input_ids'].to(DEVICE)\n",
    "    attention_mask = encoded['attention_mask'].to(DEVICE)\n",
    "    outputs = t5_mlm.generate(input_ids = input_ids, \n",
    "                          num_beams=200, num_return_sequences=1,\n",
    "                          max_length=15*total_chunk)\n",
    "\n",
    "    for i in range(len(outputs)):\n",
    "        query = t5_tokenizer.decode(outputs[i], skip_special_tokens=False, \n",
    "                                    no_repeat_ngram_size=3, early_stopping=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "        if first_token_mask is True and last_token_mask is False:\n",
    "            sent_token_id = 0\n",
    "            s = str(query)\n",
    "            start = '<pad>'\n",
    "            end = '<extra_id_0>'\n",
    "            pattern_0 = s[s.find(start)+len(start):s.rfind(end)]\n",
    "            pattern_list = []\n",
    "            pattern_list.append(pattern_0)\n",
    "            i = 0\n",
    "            while i < total_chunk-1:\n",
    "                start = f'<extra_id_{sent_token_id}>'\n",
    "                end = f'<extra_id_{sent_token_id+1}>'\n",
    "                a = s[s.find(start)+len(start):s.rfind(end)]\n",
    "                pattern_list.append(a)\n",
    "                sent_token_id += 1 \n",
    "                i +=1\n",
    "            \n",
    "            sent_token_id = 0\n",
    "            for i in range(len(pattern_list)):\n",
    "                a = f'<extra_id_{sent_token_id}>'\n",
    "                text = text.replace(a, pattern_list[i].strip())\n",
    "                sent_token_id += 1\n",
    "            gen_list.append(text.strip())\n",
    "            \n",
    "        if first_token_mask is True and last_token_mask is True:\n",
    "            sent_token_id = 0\n",
    "            s = str(query)\n",
    "            start = '<pad>'\n",
    "            end = '<extra_id_0>'\n",
    "            pattern_0 = s[s.find(start)+len(start):s.rfind(end)]\n",
    "            pattern_list = []\n",
    "            pattern_list.append(pattern_0)\n",
    "            i = 0\n",
    "            while i < total_chunk-1:\n",
    "                if i == (total_chunk -2):\n",
    "                    start = f'<extra_id_{sent_token_id}>'\n",
    "                    end = '</s>'\n",
    "                    a = s[s.find(start)+len(start):s.rfind(end)]\n",
    "                    pattern_list.append(a) \n",
    "                    i +=1\n",
    "                else:\n",
    "                    start = f'<extra_id_{sent_token_id}>'\n",
    "                    end = f'<extra_id_{sent_token_id+1}>'\n",
    "                    a = s[s.find(start)+len(start):s.rfind(end)]\n",
    "                    pattern_list.append(a)\n",
    "                    sent_token_id += 1 \n",
    "                    i +=1\n",
    "            \n",
    "            sent_token_id = 0\n",
    "            for i in range(len(pattern_list)):\n",
    "                a = f'<extra_id_{sent_token_id}>'\n",
    "                text = text.replace(a, pattern_list[i].strip())\n",
    "                sent_token_id += 1\n",
    "            gen_list.append(text.strip())\n",
    "            \n",
    "            \n",
    "        if first_token_mask is False and last_token_mask is False:\n",
    "            sent_token_id = 0\n",
    "            s = str(query)\n",
    "            pattern_list = []\n",
    "            while i < total_chunk:\n",
    "                start = f'<extra_id_{sent_token_id}>'\n",
    "                end = f'<extra_id_{sent_token_id+1}>'\n",
    "                a = s[s.find(start)+len(start):s.rfind(end)]\n",
    "                pattern_list.append(a)\n",
    "                sent_token_id += 1 \n",
    "                i +=1\n",
    "                \n",
    "            sent_token_id = 0\n",
    "            for i in range(len(pattern_list)):\n",
    "                a = f'<extra_id_{sent_token_id}>'\n",
    "                text = text.replace(a, pattern_list[i].strip())\n",
    "                sent_token_id += 1\n",
    "            gen_list.append(text.strip())    \n",
    "            \n",
    "        if first_token_mask is False and last_token_mask is True:\n",
    "            sent_token_id = 0\n",
    "            s = str(query)\n",
    "            pattern_list = []\n",
    "            while i < total_chunk:\n",
    "                if i == total_chunk -1:\n",
    "                    start = f'<extra_id_{sent_token_id}>'\n",
    "                    end = '</s>'\n",
    "                    a = s[s.find(start)+len(start):s.rfind(end)]\n",
    "                    pattern_list.append(a) \n",
    "                    i +=1\n",
    "                else:\n",
    "                    start = f'<extra_id_{sent_token_id}>'\n",
    "                    end = f'<extra_id_{sent_token_id+1}>'\n",
    "                    a = s[s.find(start)+len(start):s.rfind(end)]\n",
    "                    pattern_list.append(a)\n",
    "                    sent_token_id += 1 \n",
    "                    i +=1 \n",
    "                \n",
    "            sent_token_id = 0\n",
    "            for i in range(len(pattern_list)):\n",
    "                a = f'<extra_id_{sent_token_id}>'\n",
    "                text = text.replace(a, pattern_list[i].strip())\n",
    "                sent_token_id += 1\n",
    "            gen_list.append(text.strip())     \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "心筋症によるショックが遷延し免疫グロブリンが奏効した月経関連黄色ぶどう球菌性トキシックショック症候群の1例\n",
      "心筋症による<extra_id_0>し免疫グロブリン<extra_id_1>黄色ぶどう球菌性トキシックショック<extra_id_2>例\n",
      "心筋症による心筋症を合併し免疫グロブリン低下を伴う黄色ぶどう球菌性トキシックショック症の1例\n",
      "Toxic shock syndrome(TSS)は黄色ぶどう球菌やA群 β 溶血性連鎖球菌が産生するスーパー抗原に起因した感染性疾患である.\n",
      "Toxicshock<extra_id_0>ぶどう球菌やA群β溶血性<extra_id_1>産生するスーパー抗原<extra_id_2>\n",
      "Toxicshocksyndromeはぶどう球菌やA群β溶血性連鎖球菌が産生するスーパー抗原の総称。</s\n",
      "スーパー抗原によって循環不全や多臓器不全を呈し致命的な経過をとりうるが,心筋症を合併した報告は少ない.\n",
      "<extra_id_0>よって<extra_id_1>多<extra_id_2>呈し致命的な経過をとりうるが,<extra_id_3>し<extra_id_4>少ない.\n",
      "よってMeckel憩室は多臓器障害を呈し致命的な経過をとりうるが,肺塞栓症を合併し肺塞栓症を合併少ない.\n",
      "今回我々は月経に関連した黄色ぶどう球菌性TSSに心筋症を合併し抗菌薬治療と免疫グロブリン投与で軽快した成人の1例を経験したため報告する.\n",
      "今回我々は月経に関連<extra_id_0>ぶどう球菌<extra_id_1>心筋症を合併<extra_id_2>で軽快<extra_id_3>の<extra_id_4>経験した<extra_id_5>.\n",
      "今回我々は月経に関連した黄色ぶどう球菌による黄色心筋症を合併したので軽快した月経の症例を経験したので報告する.\n",
      "症例患者:39歳女性.主訴:発熱,頭痛.既往歴:特記すべき既往歴なし.現病歴:入院前日から発熱と頭痛があり当院外来を受診した.39.0度の発熱があるが,その他バイタルサインや身体所見,血液検査所見で特記なく対症療法で経過観察された.帰宅後も症状が続き,当院外来を再診した.\n",
      "症例患者<extra_id_0>女性.主訴:発熱<extra_id_1>既往歴:特記<extra_id_2>病歴:入院<extra_id_3>と頭痛が<extra_id_4>た.<extra_id_5>がある<extra_id_6>他<extra_id_7>身体所見,血液検査所見で特記なく対症療法で経過観察<extra_id_8>.帰宅後も<extra_id_9>,当院<extra_id_10>した.\n",
      "症例患者:77歳女性.主訴:発熱.頭痛.既往歴:特記なし.現病歴:入院1,2カ月前より倦怠感出現し 出現し喘息の既往 喘息の既往.その .その疾患: 疾患:された された症状なく 症状なくを受診<extra_id_4> </s を受診と頭痛が出現した.喘息の既往がある.その他疾患:身体所見,血液検査所見で特記なく対症療法で経過観察された.帰宅後も症状なく,当院を受診<extra_id_4> </sした.\n",
      "再診時体温40.0度,血圧100/70mmHg,脈拍数98/分(整),呼吸数16/分,SpO2 98%(室内気)で意識清明だった.\n",
      "再診<extra_id_0>,血圧100/70mmHg,脈拍数98<extra_id_1>呼吸数<extra_id_2>,SpO298%(<extra_id_3>で意識清明<extra_id_4>\n",
      "再診時意識清明,血圧100/70mmHg,脈拍数98回/分,呼吸数98回/分,SpO298%(roomair)で意識清明であった。</s\n",
      "上下肢や体幹部に明らかな紅斑なく,その他身体所見上特記すべき所見はなかった.\n",
      "上下<extra_id_0>な紅斑なく,その他身体所見上特記す<extra_id_1>\n",
      "上下肢に明らかな紅斑なく,その他身体所見上特記すべき所見なし。</s\n",
      "血液検査所見は,好中球数10,820/μL,CRP14.4mg/dLと上昇し,クレアチニン1.17mg/dLと上昇を示した.\n",
      "血液検査所見は<extra_id_0>10,820/μL<extra_id_1>mg<extra_id_2>上昇<extra_id_3>1.17mg/<extra_id_4>.\n",
      "血液検査所見は,白血球数10,820/μL,白血球数mg/dLと上昇した(1.17mg/dL).\n",
      "尿中白血球1視野に100以上と上昇を示し,胸腹部CT検査上発熱源を示唆する有意な所見や両側腎盂拡張所見がなく,急性単純性腎盂腎炎として入院となった.\n",
      "<extra_id_0>以上と上昇<extra_id_1>検査上発熱源<extra_id_2>有意な所見や両側腎盂拡張<extra_id_3>,急性<extra_id_4>腎炎<extra_id_5>入院となった.\n",
      "COVID-19は以上と上昇し,血液検査上発熱源として有意な所見や両側腎盂拡張所見はなく,急性</s腎炎-19は<extra_id_0> し,血液<extra_id_1> として<extra_id_2> 所見はなく<extra_id_3> </s入院となった.\n",
      "入院後経過:尿培養と血液培養採取後cefmetazole(CMZ)1g×3/day投与を開始された.\n",
      "入院後<extra_id_0>培養と血液<extra_id_1>cefmetazole<extra_id_2>1g×3/day投与<extra_id_3>\n",
      "入院後経過:便培養と血液培養採取後cefmetazole1g×1g×3/day投与した。</s\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences[0:10])):\n",
    "    print(sentences[i])\n",
    "    print(input_list[i])\n",
    "    print(gen_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['心筋症によるショックが遷延し免疫グロブリンが奏効した月経関連黄色ぶどう球菌性トキシックショック症候群の1例',\n",
       " 'Toxic shock syndrome(TSS)は黄色ぶどう球菌やA群 β 溶血性連鎖球菌が産生するスーパー抗原に起因した感染性疾患である.',\n",
       " 'スーパー抗原によって循環不全や多臓器不全を呈し致命的な経過をとりうるが,心筋症を合併した報告は少ない.',\n",
       " '今回我々は月経に関連した黄色ぶどう球菌性TSSに心筋症を合併し抗菌薬治療と免疫グロブリン投与で軽快した成人の1例を経験したため報告する.',\n",
       " '症例患者:39歳女性.主訴:発熱,頭痛.既往歴:特記すべき既往歴なし.現病歴:入院前日から発熱と頭痛があり当院外来を受診した.39.0度の発熱があるが,その他バイタルサインや身体所見,血液検査所見で特記なく対症療法で経過観察された.帰宅後も症状が続き,当院外来を再診した.',\n",
       " '再診時体温40.0度,血圧100/70mmHg,脈拍数98/分(整),呼吸数16/分,SpO2 98%(室内気)で意識清明だった.',\n",
       " '上下肢や体幹部に明らかな紅斑なく,その他身体所見上特記すべき所見はなかった.',\n",
       " '血液検査所見は,好中球数10,820/μL,CRP14.4mg/dLと上昇し,クレアチニン1.17mg/dLと上昇を示した.',\n",
       " '尿中白血球1視野に100以上と上昇を示し,胸腹部CT検査上発熱源を示唆する有意な所見や両側腎盂拡張所見がなく,急性単純性腎盂腎炎として入院となった.',\n",
       " '入院後経過:尿培養と血液培養採取後cefmetazole(CMZ)1g×3/day投与を開始された.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['心筋症によるショックが治療に奏効した非外傷性トキシックショック症候群の1例',\n",
       " 'ヒトメタニューモウイルス(hMPV)は,Clostridiumbactrumbactrumbactrumbactrumbactrumbactrumbactrumbactrumbac群β溶血性連鎖球菌ューモウイルス(hMPV<extra_id_0>,Clostridiumbactrumbactrumbactrumbactrumbactrumbactrumbactrumbactrumbacに起因した感染性疾患である.',\n",
       " 'スーパーインフルは循環不全や多臓器障害を伴う経過をとりうるが,心筋症を合併した報告は少ない.',\n",
       " '今回,COVID-19に関連した黄色ぶどう球菌性TSSに心筋症を合併し抗菌薬投与後に軽快した成人の症例を経験したため報告する.',\n",
       " '症例患者:39歳女性.主訴:発熱,頭痛.既往歴:特記事項なし.現病歴:入院前日から発熱と頭痛があり当院を受診した.39.発熱はあるが,その他バイタルサインや身体所見,既往歴はなく対症療法であった.帰宅後も症状が続き,当院を受診した.',\n",
       " '再診時体温40.0°C,血圧70mmHg,脈拍92回(整),呼吸数16分,SpO298%(室内気)で意識清明だった.',\n",
       " '上下肢に明らかな紅斑なく,その他身体所見上特記すべき異常所見なし。',\n",
       " '血液検査所見は,白血球数,L,CRP,クレアチニン値が上昇し,クレアチニン値もやや上昇を示した.',\n",
       " '尿中白血球1視野に100以上と上昇を示し,臨床症状上発熱源を伴う発熱源や両側腎盂拡張はなく,単純性腎盂拡張のみとなった.',\n",
       " '入院後経過:尿培養採取後にcefmetazole(CMZ)100mg/day投与を開始された.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['心筋症によるショック<extra_id_0>奏効した<extra_id_1>性トキシックショック症候群の1例',\n",
       " '<extra_id_0>)は<extra_id_1>群β溶血性連鎖球菌<extra_id_2>に起因した感染性疾患である.',\n",
       " 'スーパー<extra_id_0>不全や多臓器<extra_id_1>経過をとりうるが,<extra_id_2>報告は少ない.',\n",
       " '<extra_id_0>関連した黄色ぶどう球菌性TSSに心筋症を合併し抗菌薬<extra_id_1>軽快した成人の<extra_id_2>たため報告する.',\n",
       " '症例患者:39歳女性.主訴:発熱,頭痛.既往歴<extra_id_0>なし.現病歴:入院前日から発熱と頭痛<extra_id_1>受診した.39.<extra_id_2>あるが,その他バイタルサインや身体所見,<extra_id_3>なく対症療法で<extra_id_4>帰宅後も症状が続き<extra_id_5>した.',\n",
       " '再診時体温40.0<extra_id_0>70mmHg,<extra_id_1>(整<extra_id_2>分,SpO298%(室内気)で意識清明だった.',\n",
       " '上下肢<extra_id_0>な紅斑なく,その他身体所見上特記すべき<extra_id_1>',\n",
       " '血液検査所見は,<extra_id_0>L,CRP<extra_id_1>上昇し,クレアチニン<extra_id_2>上昇を示した.',\n",
       " '尿中白血球1視野に100以上と上昇を示し<extra_id_0>上発熱源を<extra_id_1>や両側腎盂拡張<extra_id_2>単純性腎盂<extra_id_3>となった.',\n",
       " '入院後経過:尿培養<extra_id_0>cefmetazole(CMZ)<extra_id_1>day投与を開始された.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
