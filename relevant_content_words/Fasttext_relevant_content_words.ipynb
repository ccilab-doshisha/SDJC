{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    \"\"\"\n",
    "    Sudachiによる単語の分割\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hinshi_list: List[str] = None, split_mode: str = \"C\"):\n",
    "        \"\"\"\n",
    "        :param hinshi_list: 使用する品詞のリスト. example) hinshi_list=[\"動詞\", \"名詞\", \"形容詞\"]\n",
    "        :param split_mode:\n",
    "        \"\"\"\n",
    "        split_mode_list = [\"A\", \"B\", \"C\"]\n",
    "        assert split_mode in split_mode, f\"{split_mode} is a non-existent split_mode {split_mode_list}\"\n",
    "        split_dic = {\n",
    "            \"A\": tokenizer.Tokenizer.SplitMode.A,\n",
    "            \"B\": tokenizer.Tokenizer.SplitMode.B,\n",
    "            \"C\": tokenizer.Tokenizer.SplitMode.C,\n",
    "        }\n",
    "        self.tokenizer_obj = dictionary.Dictionary().create()\n",
    "        self.mode = split_dic[split_mode]\n",
    "        self.hinshi_list = hinshi_list\n",
    "\n",
    "    def __call__(self, text: str) -> str:\n",
    "        if self.hinshi_list:\n",
    "            return \" \".join([m.normalized_form() for m in self.tokenizer_obj.tokenize(text, self.mode) if\n",
    "                             m.part_of_speech()[0] in self.hinshi_list and m.normalized_form() != \" \"])\n",
    "        return \" \".join(\n",
    "            m.normalized_form() for m in self.tokenizer_obj.tokenize(text, self.mode) if m.normalized_form() != \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(\"/home/chen/cc.ja.300.vec.gz\")\n",
    "tokenizer = Tokenizer(hinshi_list=[\"動詞\", \"名詞\", \"形容詞\"], split_mode=\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.load('ja_ginza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/chen/relevant_content_words/dataset/QAbot-relevant.csv'\n",
    "df = pd.read_csv(dataset_path, sep = '\\t', header = None, names = [\"sent1\", \"sent2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(v1, v2) -> float:\n",
    "    return np.dot(v1,v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(inputs):\n",
    "    sentvec = []\n",
    "    if inputs != None:\n",
    "        word_list = tokenizer(str(inputs)).split()\n",
    "        for word in word_list:\n",
    "            if word in model.key_to_index:\n",
    "                sentvec.append(model.get_vector(word))\n",
    "            else:\n",
    "                sentvec.append(np.random.uniform(-0.01, 0.01, model.vector_size))\n",
    "        if not sentvec:\n",
    "                sentvec.append(np.random.uniform(-0.01, 0.01, model.vector_size))\n",
    "        sentvec = np.mean(sentvec, 0)\n",
    "    else:\n",
    "        sentvec= np.random.uniform(-0.01, 0.01, model.vector_size)   \n",
    "\n",
    "    return sentvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 53/53 [00:03<00:00, 13.44it/s]\n"
     ]
    }
   ],
   "source": [
    "pos_total_list = []\n",
    "for i in tqdm(range(len(df))):\n",
    "    pos_list = []\n",
    "    cos_relevant_score = []\n",
    "    sentence1 = df['sent1'][i]\n",
    "    sentence2 = df['sent2'][i]\n",
    "    sent1_embedding = get_embeddings(sentence1)\n",
    "    sent2_embedding = get_embeddings(sentence2)\n",
    "    cossim_sent1andsent2 = cos_sim(np.squeeze(sent1_embedding), np.squeeze(sent2_embedding))\n",
    "    doc_sent1 = nlp(sentence1)\n",
    "    doc_sent2 = nlp(sentence2)\n",
    "    duplicate_tokens = []\n",
    "    for token in doc_sent1:\n",
    "        if token.text in [j.text for j in doc_sent2]:\n",
    "            duplicate_tokens.append(token.text)\n",
    "\n",
    "        re_sent1 = sentence1.replace(token.text, '')\n",
    "        re_sent2 = sentence2.replace(token.text, '')\n",
    "        \n",
    "        re_sent1_embedding = get_embeddings(re_sent1)\n",
    "        re_sent2_embedding = get_embeddings(re_sent2)\n",
    "        re_cossim_1 = cos_sim(re_sent1_embedding.squeeze(), sent2_embedding.squeeze())\n",
    "        re_cossim_2 = cos_sim(sent1_embedding.squeeze(), re_sent2_embedding.squeeze())\n",
    "        pos_list.append(token.pos_)\n",
    "        relevant_score = cossim_sent1andsent2 - min(re_cossim_1, re_cossim_2)\n",
    "        cos_relevant_score.append(relevant_score)    \n",
    "        \n",
    "    for token in doc_sent2:\n",
    "        if token.text in duplicate_tokens:\n",
    "            continue\n",
    "        else:\n",
    "            re_sent1 = sentence1.replace(token.text, '')\n",
    "            re_sent2 = sentence2.replace(token.text, '')\n",
    "            \n",
    "            re_sent1_embedding = get_embeddings(re_sent1)\n",
    "            re_sent2_embedding = get_embeddings(re_sent2)\n",
    "            re_cossim_1 = cos_sim(re_sent1_embedding.squeeze(), sent2_embedding.squeeze())\n",
    "            re_cossim_2 = cos_sim(sent1_embedding.squeeze(), re_sent2_embedding.squeeze())\n",
    "            pos_list.append(token.pos_)\n",
    "            relevant_score = cossim_sent1andsent2 - min(re_cossim_1, re_cossim_2)\n",
    "            cos_relevant_score.append(relevant_score)\n",
    "            \n",
    "    max_indexs = [index for index, item in enumerate(cos_relevant_score) if item == max(cos_relevant_score)] \n",
    "    for ii in range(len(max_indexs)):\n",
    "        idx = max_indexs[ii]\n",
    "        pos_total_list.append(pos_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_total_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(lists):\n",
    "    list_set = set(lists)\n",
    "    unique_list = (list(list_set))\n",
    "    for x in unique_list:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJ\n",
      "SCONJ\n",
      "ADP\n",
      "PART\n",
      "AUX\n",
      "NUM\n",
      "NOUN\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "unique(pos_total_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PART_num, SYM_num, X_num, AUX_num, ADP_num, ADV_num, ADJ_num, NUM_num = 0,0,0,0,0,0,0,0\n",
    "SCONJ_num, PUNCT_num, PROPN_num, NOUN_num, VERB_num, CCONJ_num, DET_num = 0,0,0,0,0,0,0\n",
    "for i in range(len(pos_total_list)):\n",
    "    if pos_total_list[i] == \"SYM\":\n",
    "        SYM_num+=1\n",
    "    elif pos_total_list[i] == \"X\":\n",
    "        X_num+=1\n",
    "    elif pos_total_list[i] == \"AUX\":\n",
    "        AUX_num+=1\n",
    "    elif pos_total_list[i] == \"ADP\":\n",
    "        ADP_num+=1\n",
    "    elif pos_total_list[i] == \"ADV\":\n",
    "        ADV_num+=1\n",
    "    elif pos_total_list[i] == \"ADJ\":\n",
    "        ADJ_num+=1\n",
    "    elif pos_total_list[i] == \"NUM\":\n",
    "        NUM_num+=1\n",
    "    elif pos_total_list[i] == \"SCONJ\":\n",
    "        SCONJ_num+=1\n",
    "    elif pos_total_list[i] == \"PUNCT\":\n",
    "        PUNCT_num+=1\n",
    "    elif pos_total_list[i] == \"PROPN\":\n",
    "        PROPN_num+=1\n",
    "    elif pos_total_list[i] == \"NOUN\":\n",
    "        NOUN_num+=1\n",
    "    elif pos_total_list[i] == \"VERB\":\n",
    "        VERB_num+=1\n",
    "    elif pos_total_list[i] == \"PART\":\n",
    "        PART_num+=1\n",
    "    elif pos_total_list[i] == \"CCONJ\":\n",
    "        CCONJ_num+=1\n",
    "    elif pos_total_list[i] == \"DET\":\n",
    "        DET_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN_per:25.00\n"
     ]
    }
   ],
   "source": [
    "print('NOUN_per:'+\"%.2f\" %(NOUN_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('SYM_per:'+\"%.2f\" %(SYM_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('X_per:'+\"%.2f\" %(X_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('AUX_per:'+\"%.2f\" %(AUX_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('ADP_per:'+\"%.2f\" %(ADP_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('ADV_per:'+\"%.2f\" %(ADV_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('ADJ_per:'+\"%.2f\" %(ADJ_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('NUM_per:'+\"%.2f\" %(NUM_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('SCONJ_per:'+\"%.2f\" %(SCONJ_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('PUNCT_per:'+\"%.2f\" %(PUNCT_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('PROPN_per:'+\"%.2f\" %(PROPN_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('NOUN_per:'+\"%.2f\" %(NOUN_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('VERB_per:'+\"%.2f\" %(VERB_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('PART_per:'+\"%.2f\" %(PART_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('CCONJ_per:'+\"%.2f\" %(CCONJ_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SYM_per:0.00',\n",
       " 'X_per:0.00',\n",
       " 'AUX_per:21.88',\n",
       " 'ADP_per:1.56',\n",
       " 'ADV_per:0.00',\n",
       " 'ADJ_per:3.12',\n",
       " 'NUM_per:15.62',\n",
       " 'SCONJ_per:10.94',\n",
       " 'PUNCT_per:0.00',\n",
       " 'PROPN_per:0.00',\n",
       " 'NOUN_per:25.00',\n",
       " 'VERB_per:20.31',\n",
       " 'PART_per:1.56',\n",
       " 'CCONJ_per:0.00']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = 'Fasttext_QAbot_relevantwords.txt'\n",
    "with open(file_out, 'w', encoding = 'utf-8') as f:\n",
    "    for i in range(len(results)):\n",
    "        f.write(results[i].strip() + '\\n')\n",
    "     \n",
    "        \n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
