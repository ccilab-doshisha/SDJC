{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#CoSENT Model\n",
    "\n",
    "from torch import nn\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.config = BertConfig.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "        self.bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese', config=self.config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, encoder_type='last-avg'):\n",
    "        '''\n",
    "        :param input_ids:\n",
    "        :param attention_mask:\n",
    "        :param encoder_type: encoder_type:  \"first-last-avg\", \"last-avg\", \"cls\", \"pooler(cls + dense)\"\n",
    "        :return:\n",
    "        '''\n",
    "        output = self.bert(input_ids, attention_mask, output_hidden_states=True)\n",
    "\n",
    "        if encoder_type == 'fist-last-avg':\n",
    "            # 第一层和最后一层的隐层取出  然后经过平均池化\n",
    "            first = output.hidden_states[1]   # hidden_states列表有13个hidden_state，第一个其实是embeddings，第二个元素才是第一层的hidden_state\n",
    "            last = output.hidden_states[-1]\n",
    "            seq_length = first.size(1)   # 序列长度\n",
    "\n",
    "            first_avg = torch.avg_pool1d(first.transpose(1, 2), kernel_size=seq_length).squeeze(-1)  # batch, hid_size\n",
    "            last_avg = torch.avg_pool1d(last.transpose(1, 2), kernel_size=seq_length).squeeze(-1)  # batch, hid_size\n",
    "            final_encoding = torch.avg_pool1d(torch.cat([first_avg.unsqueeze(1), last_avg.unsqueeze(1)], dim=1).transpose(1, 2), kernel_size=2).squeeze(-1)\n",
    "            return final_encoding\n",
    "\n",
    "        if encoder_type == 'last-avg':\n",
    "            sequence_output = output.last_hidden_state  # (batch_size, max_len, hidden_size)\n",
    "            seq_length = sequence_output.size(1)\n",
    "            final_encoding = torch.avg_pool1d(sequence_output.transpose(1, 2), kernel_size=seq_length).squeeze(-1)\n",
    "            return final_encoding\n",
    "\n",
    "        if encoder_type == \"cls\":\n",
    "            sequence_output = output.last_hidden_state\n",
    "            cls = sequence_output[:, 0]  # [b,d]\n",
    "            return cls\n",
    "\n",
    "        if encoder_type == \"pooler\":\n",
    "            pooler_output = output.pooler_output  # [b,d]\n",
    "            return pooler_output\n",
    "        \n",
    "        \n",
    "model = Model()\n",
    "model.load_state_dict(torch.load('/home/chen/CoSENT_Pytorch/outputs/base_model_epoch_4.bin'))\n",
    "tokenizer = AutoTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = '/home/chen/SimCSE/result/my-sup-simcse-bert-large-hard_neg1-batch-512-stsb'\n",
    "model = AutoModel.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.load('ja_ginza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/chen/relevant_content_words/dataset/QAbot-relevant.csv'\n",
    "df = pd.read_csv(dataset_path, sep = '\\t', header = None, names = [\"sent1\", \"sent2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(v1, v2) -> float:\n",
    "    return np.dot(v1,v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#CoSENT Model\n",
    "def get_embeddings(inputs):\n",
    "    with torch.no_grad():\n",
    "            outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "    return outputs.cpu()        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#first-last-avg\n",
    "def get_embeddings(inputs):\n",
    "    with torch.no_grad():\n",
    "            outputs = model(**inputs, return_dict=True, output_hidden_states=True)\n",
    "            last_hidden = outputs.last_hidden_state\n",
    "            pooler_output = outputs.pooler_output\n",
    "            hidden_states = outputs.hidden_states\n",
    "    first_hidden = hidden_states[0]\n",
    "    last_hidden = hidden_states[-1]\n",
    "    pooled_result = ((first_hidden + last_hidden) / 2.0 * inputs['attention_mask'].unsqueeze(-1)).sum(1) / inputs['attention_mask'].sum(-1).unsqueeze(-1)\n",
    "    return pooled_result.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean embedding\n",
    "def get_embeddings(inputs):\n",
    "    with torch.no_grad():\n",
    "            outputs = model(**inputs, return_dict=True)\n",
    "            last_hidden = outputs.last_hidden_state\n",
    "            pooler_output = outputs.pooler_output\n",
    "            \n",
    "    return ((last_hidden * inputs['attention_mask'].unsqueeze(-1)).sum(1) / inputs['attention_mask'].sum(-1).unsqueeze(-1)).cpu()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 53/53 [01:15<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "pos_total_list = []\n",
    "for i in tqdm(range(len(df))):\n",
    "    pos_list = []\n",
    "    cos_relevant_score = []\n",
    "    sentence1 = df['sent1'][i]\n",
    "    sentence2 = df['sent2'][i]\n",
    "    sent1_tk = tokenizer(sentence1, return_tensors = 'pt', padding = True, max_length = 32, truncation = True)\n",
    "    sent2_tk = tokenizer(sentence2, return_tensors = 'pt', padding = True, max_length = 32, truncation = True)\n",
    "    sent1_tk = sent1_tk.to(device)\n",
    "    sent2_tk = sent2_tk.to(device)\n",
    "    sent1_embedding = get_embeddings(sent1_tk)\n",
    "    sent2_embedding = get_embeddings(sent2_tk)\n",
    "    cossim_sent1andsent2 = cos_sim(np.squeeze(sent1_embedding), np.squeeze(sent2_embedding))\n",
    "    doc_sent1 = nlp(sentence1)\n",
    "    doc_sent2 = nlp(sentence2)\n",
    "    duplicate_tokens = []\n",
    "    for token in doc_sent1:\n",
    "        if token.text in [j.text for j in doc_sent2]:\n",
    "            duplicate_tokens.append(token.text)\n",
    "\n",
    "        re_sent1 = sentence1.replace(token.text, '')\n",
    "        re_sent2 = sentence2.replace(token.text, '')\n",
    "            \n",
    "        re_sent1_tk = tokenizer(re_sent1, return_tensors = 'pt', \n",
    "                                    padding = True, max_length = 32, truncation = True)\n",
    "        re_sent2_tk = tokenizer(re_sent2, return_tensors = 'pt', \n",
    "                                    padding = True, max_length = 32, truncation = True)\n",
    "        \n",
    "        re_sent1_tk = re_sent1_tk.to(device)\n",
    "        re_sent2_tk = re_sent2_tk.to(device)\n",
    "        \n",
    "        re_sent1_embedding = get_embeddings(re_sent1_tk)\n",
    "        re_sent2_embedding = get_embeddings(re_sent2_tk)\n",
    "        re_cossim_1 = cos_sim(re_sent1_embedding.squeeze(), sent2_embedding.squeeze())\n",
    "        re_cossim_2 = cos_sim(sent1_embedding.squeeze(), re_sent2_embedding.squeeze())\n",
    "        pos_list.append(token.pos_)\n",
    "        relevant_score = cossim_sent1andsent2 - min(re_cossim_1, re_cossim_2)\n",
    "        cos_relevant_score.append(relevant_score)    \n",
    "        \n",
    "    for token in doc_sent2:\n",
    "        if token.text in duplicate_tokens:\n",
    "            continue\n",
    "        else:\n",
    "            re_sent1 = sentence1.replace(token.text, '')\n",
    "            re_sent2 = sentence2.replace(token.text, '')\n",
    "            \n",
    "            re_sent1_tk = tokenizer(re_sent1, return_tensors = 'pt', \n",
    "                                    padding = True, max_length = 32, truncation = True)\n",
    "            re_sent2_tk = tokenizer(re_sent2, return_tensors = 'pt', \n",
    "                                    padding = True, max_length = 32, truncation = True)\n",
    "            \n",
    "            re_sent1_tk = re_sent1_tk.to(device)\n",
    "            re_sent2_tk = re_sent2_tk.to(device)\n",
    "            \n",
    "            re_sent1_embedding = get_embeddings(re_sent1_tk)\n",
    "            re_sent2_embedding = get_embeddings(re_sent2_tk)\n",
    "            re_cossim_1 = cos_sim(re_sent1_embedding.squeeze(), sent2_embedding.squeeze())\n",
    "            re_cossim_2 = cos_sim(sent1_embedding.squeeze(), re_sent2_embedding.squeeze())\n",
    "            pos_list.append(token.pos_)\n",
    "            relevant_score = cossim_sent1andsent2 - min(re_cossim_1, re_cossim_2)\n",
    "            cos_relevant_score.append(relevant_score)\n",
    "            \n",
    "    max_indexs = [index for index, item in enumerate(cos_relevant_score) if item == max(cos_relevant_score)] \n",
    "    for ii in range(len(max_indexs)):\n",
    "        idx = max_indexs[ii]\n",
    "        pos_total_list.append(pos_list[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_total_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AUX',\n",
       " 'ADP',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'ADP',\n",
       " 'ADP',\n",
       " 'AUX',\n",
       " 'VERB',\n",
       " 'AUX',\n",
       " 'NOUN',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'AUX',\n",
       " 'NOUN',\n",
       " 'NOUN',\n",
       " 'AUX',\n",
       " 'ADP']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_total_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(lists):\n",
    "    list_set = set(lists)\n",
    "    unique_list = (list(list_set))\n",
    "    for x in unique_list:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VERB\n",
      "ADP\n",
      "PROPN\n",
      "ADJ\n",
      "X\n",
      "NOUN\n",
      "NUM\n",
      "PUNCT\n"
     ]
    }
   ],
   "source": [
    "unique(pos_total_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'other'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'symbol'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"SYM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PART_num, SYM_num, X_num, AUX_num, ADP_num, ADV_num, ADJ_num, NUM_num = 0,0,0,0,0,0,0,0\n",
    "SCONJ_num, PUNCT_num, PROPN_num, NOUN_num, VERB_num, CCONJ_num, PRON_num = 0,0,0,0,0,0,0\n",
    "for i in range(len(pos_total_list)):\n",
    "    if pos_total_list[i] == \"SYM\":\n",
    "        SYM_num+=1\n",
    "    elif pos_total_list[i] == \"X\":\n",
    "        X_num+=1\n",
    "    elif pos_total_list[i] == \"AUX\":\n",
    "        AUX_num+=1\n",
    "    elif pos_total_list[i] == \"ADP\":\n",
    "        ADP_num+=1\n",
    "    elif pos_total_list[i] == \"ADV\":\n",
    "        ADV_num+=1\n",
    "    elif pos_total_list[i] == \"ADJ\":\n",
    "        ADJ_num+=1\n",
    "    elif pos_total_list[i] == \"NUM\":\n",
    "        NUM_num+=1\n",
    "    elif pos_total_list[i] == \"SCONJ\":\n",
    "        SCONJ_num+=1\n",
    "    elif pos_total_list[i] == \"PUNCT\":\n",
    "        PUNCT_num+=1\n",
    "    elif pos_total_list[i] == \"PROPN\":\n",
    "        PROPN_num+=1\n",
    "    elif pos_total_list[i] == \"NOUN\":\n",
    "        NOUN_num+=1\n",
    "    elif pos_total_list[i] == \"VERB\":\n",
    "        VERB_num+=1\n",
    "    elif pos_total_list[i] == \"PART\":\n",
    "        PART_num+=1\n",
    "    elif pos_total_list[i] == \"CCONJ\":\n",
    "        CCONJ_num+=1\n",
    "    elif pos_total_list[i] == \"PRON\":\n",
    "        PRON_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN_per:56.45\n"
     ]
    }
   ],
   "source": [
    "print('NOUN_per:'+\"%.2f\" %(NOUN_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('SYM_per:'+\"%.2f\" %(SYM_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('X_per:'+\"%.2f\" %(X_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('AUX_per:'+\"%.2f\" %(AUX_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('ADP_per:'+\"%.2f\" %(ADP_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('ADV_per:'+\"%.2f\" %(ADV_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('ADJ_per:'+\"%.2f\" %(ADJ_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('NUM_per:'+\"%.2f\" %(NUM_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('SCONJ_per:'+\"%.2f\" %(SCONJ_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('PUNCT_per:'+\"%.2f\" %(PUNCT_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('PROPN_per:'+\"%.2f\" %(PROPN_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('NOUN_per:'+\"%.2f\" %(NOUN_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('VERB_per:'+\"%.2f\" %(VERB_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('PART_per:'+\"%.2f\" %(PART_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('CCONJ_per:'+\"%.2f\" %(CCONJ_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append('PRON_per:'+\"%.2f\" %(PRON_num / len(pos_total_list) *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SYM_per:0.00',\n",
       " 'X_per:3.23',\n",
       " 'AUX_per:0.00',\n",
       " 'ADP_per:3.23',\n",
       " 'ADV_per:0.00',\n",
       " 'ADJ_per:9.68',\n",
       " 'NUM_per:1.61',\n",
       " 'SCONJ_per:0.00',\n",
       " 'PUNCT_per:6.45',\n",
       " 'PROPN_per:14.52',\n",
       " 'NOUN_per:56.45',\n",
       " 'VERB_per:4.84',\n",
       " 'PART_per:0.00',\n",
       " 'CCONJ_per:0.00',\n",
       " 'PRON_per:0.00']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_out = 'SimCSE_large_sup_QAbot_relevantwords.txt'\n",
    "with open(file_out, 'w', encoding = 'utf-8') as f:\n",
    "    for i in range(len(results)):\n",
    "        f.write(results[i].strip() + '\\n')\n",
    "     \n",
    "        \n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
