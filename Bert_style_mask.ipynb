{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def racha_detection(lista):\n",
    "    # It returns a list of lists where each sub-list contains the consecutive tokens in the list\n",
    "    rachas = []\n",
    "    racha = []\n",
    "    for i, element in enumerate(lista):\n",
    "        if (i<len(lista)-1) and (lista[i+1] == element+1):\n",
    "            racha.append(element)\n",
    "        else:\n",
    "            if len(racha)>0:\n",
    "                rachas.append(racha + [element])          \n",
    "            else:# (i!=len(lista)-1):\n",
    "                rachas.append([element])\n",
    "            racha = []\n",
    "    return rachas\n",
    "\n",
    "def masking(tokenized_sentence, rachas):\n",
    "    # Function to mask a tokenized_sentence (token ids) following the rachas described in rachas\n",
    "    # Only one sentinel_token per racha\n",
    "    sent_token_id = 0\n",
    "    enmascared = tokenized_sentence.copy()\n",
    "    for racha in rachas:\n",
    "        sent_token = f'<extra_id_{sent_token_id}>'\n",
    "        sent_id = tokenizer.encode(sent_token)[0]\n",
    "        for i, idx in enumerate(racha):\n",
    "            if i==0:\n",
    "                enmascared[idx] = sent_id\n",
    "            else:\n",
    "                enmascared[idx] = -100\n",
    "        sent_token_id += 1\n",
    "        \n",
    "    enmascared = [t for t in enmascared if t!=-100] \n",
    "\n",
    "    return enmascared\n",
    "\n",
    "def add_noise(sentence, tokenizer, percent=0.15):\n",
    "    # Function that takes a sentence, tokenizer and a noise percentage and returns\n",
    "    # the masked input_ids and masked target_ids accordling with the T5 paper and HuggingFace docs\n",
    "    # To see the process working uncomment all the prints ;)\n",
    "    tokenized_sentence = tokenizer.encode(sentence)\n",
    "    #print('PRE-MASKED:')\n",
    "    #print('INPUT: {}'.format(tokenizer.convert_ids_to_tokens(tokenized_sentence)))\n",
    "   \n",
    "    idxs_2_mask = sorted(random.sample(range(len(tokenized_sentence)), \n",
    "                                       int(len(tokenized_sentence)*percent)))\n",
    "    rachas = racha_detection(idxs_2_mask)\n",
    "    enmascared_input = masking(tokenized_sentence, rachas)\n",
    "    #print('RACHAS INPUT: {}'.format(rachas))\n",
    "    idxs_2_mask = [idx for idx in range(len(tokenized_sentence)) if idx not in idxs_2_mask]\n",
    "    rachas = racha_detection(idxs_2_mask)\n",
    "    enmascared_target = masking(tokenized_sentence, rachas)\n",
    "    #print('RACHAS TARGET: {}'.format(rachas))\n",
    "    \n",
    "    #print('POST-MASKED:')\n",
    "    #print('INPUT: {}'.format(tokenizer.convert_ids_to_tokens(enmascared_input)))\n",
    "    #print('TARGET: {}'.format(tokenizer.convert_ids_to_tokens(enmascared_target)))\n",
    "\n",
    "    return enmascared_input, enmascared_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
    "\n",
    "T5_PATH = 'sonoisa/t5-base-japanese-v1.1'\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(T5_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = t5_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Toxic shock syndrome(TSS)は黄色ぶどう球菌やA群 β 溶血性連鎖球菌が産生するスーパー抗原に起因した感染性疾患である.'\n",
    "\n",
    "outputs = add_noise(text, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: ['▁', 'T', 'ox', 'ic', '▁', '<extra_id_0>', 'syn', 'dr', 'ome', '(', 'T', '<0x53>', '<extra_id_1>', ')', 'は', '黄色', '<extra_id_2>', 'どう', '球', '菌', 'や', '<extra_id_3>', '群', '▁', 'β', '▁', '溶', '血', '性', '連鎖', '球', '菌', 'が', '産', '生', 'する', 'スーパー', '抗', '原', 'に', '起', '因', 'した', '<extra_id_4>', '性', '疾患', 'である', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print('INPUT: {}'.format(tokenizer.convert_ids_to_tokens(outputs[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET: ['<extra_id_0>', 'sho', 'ck', '▁', '<extra_id_1>', '<0x53>', '<extra_id_2>', 'ぶ', '<extra_id_3>', '<0x41>', '<extra_id_4>', '感染', '<extra_id_5>']\n"
     ]
    }
   ],
   "source": [
    "print('TARGET: {}'.format(tokenizer.convert_ids_to_tokens(outputs[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Toxicshocksyndrome(TSS)は黄色ぶどう球菌やA群β溶血性連鎖球菌が産生するスーパー抗原に起因した感染性疾患である.'\n",
    "\n",
    "outputs = add_noise(text, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: ['▁', '<extra_id_0>', 'ox', 'ic', 'sho', 'ck', 'syn', 'dr', 'ome', '(', 'T', '<0x53>', '<0x53>', ')', 'は', '黄色', 'ぶ', 'どう', '球', '菌', 'や', '<0x41>', '群', 'β', '溶', '血', '<extra_id_1>', '菌', '<extra_id_2>', '生', 'する', '<extra_id_3>', '抗', '原', 'に', '起', '因', 'した', '感染', '性', '疾患', 'である', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print('INPUT: {}'.format(tokenizer.convert_ids_to_tokens(outputs[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
